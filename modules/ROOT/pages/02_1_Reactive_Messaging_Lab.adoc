:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Reactive Messaging Lab

.Prerequisites
* Access to the Red Hat^(R)^ OpenShift^(R)^ Container Platform 4.x cluster
* Prerequisites for local workstation:
** Java development environment for Java 11, including an IDE
** Development tools: git, Maven (version 3.6.3)
** Tools for container image management (Docker, Podman)
** API Testing tools (Postman, SoapUI)
** Ansible (version >= 2.9.0)
** Openshift `oc` CLI client, version 4.6.x

.Goals
* Understand reactive messaging with Quarkus
* Use reactive messaging with Quarkus to consume and produce messages to and from a Kafka cluster

:numbered:

== Reactive Messaging

The Eclipse MicroProfile Reactive Messaging specification defines a framework for building event-driven, data streaming, and event-sourcing applications.

The MicroProfile Reactive Messaging specification aims to deliver applications embracing the characteristics of reactive systems: responsive, resilient, elastic and asynchronous.

SmallRye Reactive Messaging is an implementation of the Eclipse MicroProfile Reactive Messaging specification. It is used in Quarkus. It lets your applications interact using various messaging technologies such as Apache Kafka, AMQP or MQTT. The framework provides a flexible programming model bridging CDI and event-driven.

*Architecture*

An application using Reactive Messaging is composed of CDI beans consuming, producing and processing messages. These messages can be wholly _internal_ to the application or can be sent and received via different message brokers. +
The applicationâ€™s beans contain methods annotated with `@Incoming` and `@Outgoing` annotations. A method with an `@Incoming` annotation consumes messages from a _channel_. A method with an `@Outgoing` annotation publishes messages to a _channel_. A method with both an `@Incoming` and an `@Outgoing` annotation is a message processor, it consumes messages from a _channel_, does some transformation to them, and publishes messages to another _channel_.

*Channel*

A _channel_ is a name indicating which source or destination of messages is used. There are two types of channel:

* Internal channels are local to the application. They allow implementing multi-step processing where several _beans_ from the same application form a chain of processing.
* Channels can be connected to remote brokers or various message transport layers such as Apache Kafka or an AMQP broker. These channels are managed by _connectors_.

image::images/reactive-messaging-channels.png[]

*Messages, Payload, Metadata*

A Message is an envelope around a payload. Your application is going to receive, process and send Messages.

These Messages can be generated by your application, or are retrieved from a message broker. They can also be consumed by your application, or sent to a message broker.

In Reactive Messaging, Message are represented by the `Message` class. Each `Message<T>` contains a payload of type `<T>`. 

Messages can also have metadata. Metadata are a way to extend messages with additional data. It can be metadata related to the message broker (`KafkaMessageMetadata` for instance), or contain operational data (such as tracing metadata), or business-related data.

image::images/reactive-messaging-messages.png[]

*Connectors*

Your application is interacting with messaging brokers using _connectors_. A connector is a piece of code that connects to a broker and:

* subscribe / poll / receive messages from the broker and propagate them to the application
* send / write / dispatch messages provided by the application to the broker

Connectors are configured to map incoming messages to a specific _channel_ (consumed by the application) and to collect outgoing messages sent to a specific _channel_ by the application. These collected messages are sent to the external broker.

Each connector is dedicated to a technology. For example a Kafka Connector only deals with Kafka.

image::images/reactive-messaging-connectors.png[]

For more details about Eclipse MicroProfile Reactive Messaging and the SmallRye implementation, visit the following sites:

* https://smallrye.io/smallrye-reactive-messaging/smallrye-reactive-messaging/2.2/index.html
* https://download.eclipse.org/microprofile/microprofile-reactive-messaging-1.0/microprofile-reactive-messaging-spec.pdf


== Implement Reactive Messaging with Quarkus

=== Implement the Message Processing Pipeline

The mission service manages the lifecycle of _mission_ entities in the Emergency Response application. A mission represents a responder picking up an group of evacuees (represented by an _incident_ entity) and bringing them to a shelter. +
The process service matches the incident to a responder and a shelter destination based on a number of criteria like number of evacuees, need for medical help, the boat capacity of the responder and the distance between the responder, the incident and the shelter. Once a suitable match is found, the process service emits a _CreateMissionCommand_ message containing the details of the mission to create - Ids of responder and incident, coordinates of the incident, responder and shelter locations.

In this lab, you implement part of the functionality for the mission service. The requirement to implement is the following:

* The mission service consumes _CreateMissionCommand_ messages from a topic on the Kafka broker.
* The payload of the message is validated. If it is not a _CreateMissionCommand_ message, the message is ignored.
* If the message is valid, the payload is deserialized into a _Mission_ object.
* The mission service calls the route planner service to obtain the route between the responder location, incident location and shelter. The Emergency Response application uses the MapBox API for this (https://www.mapbox.com). The route consists of a list of _MissionStep_ instances. The list is added to the mission object.
* The status of the mission is set to `CREATED`.
* The mission object is stored in a repository. The Emergency Response application uses Red Hat DataGrid as repository for mission objects.
* A _MissionStartedEvent_ message is created and sent to a Kafka topic.

The mission service is implemented with Quarkus, and uses Quarkus Reactive messaging to consume and produce Kafka messages. In this lab, you start with an application skeleton with stubs for the route planner service and the mission repository.

. Check out the code for the mission service:
+
----
$ git clone https://github.com/gpte-cloud-native-advanced/mission-service-reactive-messaging.git
----
. Import the code into your IDE of choice.
. Ensure the code builds correctly:
+
----
$ mvn clean package
----
. Review the code. There is not a lot to it. The `model` packages contains the data model classes. `RoutePlanner` and `MissionRepository` are stubs for the route planner service and the repository service. There are a couple of tests for the serialization an deserialization of mission entities.

. Add a dependency to the Quarkus `quarkus-smallrye-reactive-messaging-kafka` extension in the `pom.xml` of the project.
+
----
   <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
    </dependency>
----

. Start by creating a class for the functionality to implement. Name the class `MissionCommandSource` to reflect that this class contains the code to execute when a `CreateMissionCommand` message is consumed.
+
----
package com.redhat.emergency.response.mission.source;

public class MissionCommandSource {
    
}
----
. Create a method to process incoming `CreateMissionCommand` messages. You need to pick some choices here:
* Method signature: with reactive messaging, there are a number of possible method signatures whether the method consumes messages, produces messages or combines both consuming and producing. 
* Message payload consumption. The payload can be consumed as the envelope (an instance of `org.eclipse.microprofile.reactive.messaging.Message`), or you can directly consume the payload itself.
* Payload deserialization. In the Emergency Response demo, events and commands are sent as JSON strings. On the consumer side, you can consume the payload as a string, or have the payload deserialized by the Kafka client to a POJO object using a JSON-aware deserializer.
* In the context of this lab, you deserialize the message to a string, consume the message envelop and return an instance of `io.smallrye.mutiny.Uni`, which represents a lazy asynchronous action. But feel free to experiment with other method signatures.
* Create a method with the following signature:
+
----
    public Uni<Message<String>> process(Message<String> missionCommandMessage) {
        return null;
    }
----
+
Make sure to use `Message` from the `org.eclipse.microprofile.reactive.messaging` package.

. The first thing to do is to verify that the incoming message has the expected structure. If not, the message can be ignored. There are several way to do so. In a later lab in this course you will see how to leverage JSON Schema to validate message payload. In this lab, you will verify that the message is a JSON string, and has an attribute called `messageType` with value `CreateMissionCommand`
+
[NOTE]
====
In the Emergency Response application, most messages that are exchanged between services have the same structure. What distinguishes message types from each other is the `messageType` attribute, which describes the type of the message, like for example _CreateMissionCommand_ or _IncidentReportedEvent_, and the `body`, which might be different for every message type. +
As an example, a _CreateMissionCommand_ message looks like:

----
{
    "id": "91cf5e82-8135-476d-ade4-5fe00dca2cc6",
    "messageType": "CreateMissionCommand",
    "invokingService": "IncidentProcessService",
    "timestamp": 1593363522344,
    "body": {
            "incidentId": "error141",
            "responderId": "responder123",
            "responderStartLat": "40.12345",
            "responderStartLong": "-80.98765",
            "incidentLat": "30.12345",
            "incidentLong": "-70.98765",
            "destinationLat": "50.12345",
            "destinationLong": "-90.98765",
            "processId": "0"
    }
}
----
====
+
Create a method `accept(String message)` which returns `Optional<JsonObject>` and with the following implementation:
+
----
    private Optional<JsonObject> accept(String messageAsJson) {
        try {
            JsonObject json = new JsonObject(messageAsJson);
            String messageType = json.getString("messageType");
            if ("CreateMissionCommand".equals(messageType) && json.containsKey("body")) {
                return Optional.of(json);
            }
            log.debug("Message with type '" + messageType + "' is ignored");
        } catch (Exception e) {
            log.warn("Unexpected message which is not JSON or without 'messageType' field.");
            log.warn("Message: " + messageAsJson);
        }
        return Optional.empty();
    }
----
+
* This method uses the `io.vertx.core.JsonObject` API to transform the string message payload into a representation of a JSON object. `JsonObject` is a very lightweight API for the manipulation of JSON objects, which originally was developed for the Vert.x project. But you can also use another JSON deserialization framework if you like.
* If the message has the expected structure, the `JsonObject` instance is returned, wrapped into an `Optional`. Otherwise the method returns an empty `Optional`.

. Create a method to validate that the body of the JSON message has all the required data. If not, ignore the message. Notice that we use the Vert.x `JsonObject` API to deserialize the JSON object into a `Mission` object. In case of a deserialization error or missing data we simply ignore the message.
+
----
    private Optional<Mission> validate(JsonObject json) {
        try {
            Optional<Mission> mission = Optional.of(json.mapTo(Mission.class))
                    .filter(m -> m.getIncidentId() != null && !(m.getIncidentId().isBlank()))
                    .filter(m -> m.getResponderId() != null && !(m.getIncidentId().isBlank()))
                    .filter(m -> m.getIncidentLat() != null && m.getIncidentLong() != null)
                    .filter(m -> m.getResponderStartLat() != null && m.getResponderStartLong() != null)
                    .filter(m -> m.getDestinationLat() != null && m.getDestinationLong() != null);
            if (mission.isEmpty()) {
                log.warn("Missing data in Mission object. Ignoring.");
            }
            return mission;
        } catch (Exception e) {
            log.error("Exception when deserializing message body into Mission object:", e);
        }
        return Optional.empty();
    }
----

. Once the message has been validated, you can implement the rest of the required functionality. The nice thing about a reactive programming style is that it is very easy to build a pipeline of actions. 
* Inject an instance of `RoutePlanner` and `MissionRepository` into the class:
+
----
    @Inject
    RoutePlanner routeplanner;

    @Inject
    MissionRepository repository;
----
* A possible implementation for the process method could look like:
+
----
    public Uni<Message<String>> process(Message<String> missionCommandMessage) {

        return Uni.createFrom().item(missionCommandMessage)
                .onItem().transform(mcm -> accept(missionCommandMessage.getPayload()))
                .onItem().transform(o -> o.flatMap(j -> validate(j.getJsonObject("body"))).orElseThrow(() -> new IllegalStateException("Message ignored")))
                .onItem().transform(m -> m.status(MissionStatus.CREATED))
                .onItem().transformToUni(m -> routeplanner.getDirections(m.responderLocation(), m.destinationLocation(), m.incidentLocation())
                        .map(missionSteps -> {
                            m.getSteps().addAll(missionSteps);
                            return m;
                        }))
                .onItem().transform(m -> {
                    repository.put(m);
                    return m;
                })
                .onItem().transform(m -> {
                    JsonObject message = new JsonObject().put("id", UUID.randomUUID().toString())
                            .put("invokingService", "MissionService")
                            .put("timestamp", Instant.now().toEpochMilli())
                            .put("messageType", "MissionStartedEvent")
                            .put("body", JsonObject.mapFrom(m));
                    return (Message<String>) KafkaRecord.of(m.getIncidentId(), message.encode());
                })
                .onFailure().recoverWithUni(() -> Uni.createFrom().nullItem());
    }
----
+
* If the validation steps fail, the pipeline throws an exception which is handled in the `onFailure` step. All other steps in the pipeline are skipped.
* To transform an item in the reactive pipeline, you use `transform(Function<? super T, ? extends R> mapper)` which returns a `Uni<R>` instance.
* The `Routeplanner.getDirections()` method is implemented as an asynchronous method which returns a instance of `Uni`. The `transformToUni(Function<? super T, ? extends Uni<? extends R>> mapper)` transforms the received item asynchronously, and forwards the events emitted by another `Uni` which is produced by the given mapper.
* At the end of the pipeline, you build a _MissionStartedEvent_ event and wrap it into a message.
* Notice the you use `KafkaRecord` which is a subtype of `org.eclipse.microprofile.reactive.messaging.Message`. `KafkaRecord` has some additional methods that allow for instance to define a key for the outgoing Kafka message. In this case you use the incident Id as Kafka message key.
* The `onFailure` exception handling block returns an empty stream, which means the outgoing channel will skip the message. +
Note that it is important to actually handle the exception. If the exception bubbles up to the Reactive Messaging Kafka connector it would cause the connector to shut down and stop consuming messages.

. If you are interested in the metadata of the incoming message, you can add code like:
+
----
        Optional<IncomingKafkaRecordMetadata> metadata = missionCommandMessage.getMetadata(IncomingKafkaRecordMetadata.class);
        metadata.ifPresent(m -> log.debug("Consumed message from topic '" + m.getTopic() + "'', partition:offset '" + m.getPartition() + ":" + m.getOffset() + "'"));
----
 
. Annotate the `process` method with Reactive Messaging annotations to indicate the consumed and populated channel:
+
----
    @Incoming("mission-command")
    @Outgoing("mission-event")
    @Acknowledgment(Acknowledgment.Strategy.PRE_PROCESSING)
    public Uni<Message<String>> process(Message<String> missionCommandMessage) {
        [...]
    }
----
+
* The `@Acknowledgement` annotation defines which message acknowledgement strategy to use. Which strategies are available depends on the method signature. In the case of `@Outgoing @Incoming Uni<Message<O>> method(Message<I> msg)`, the only possibilities are `MANUAL` or `PRE_PROCESSING`. `PRE_PROCESSING` means that acknowledgement is done before the processing pipeline is invoked. +
When using Kafka, acknowledgement commits the message offset. +

. Finally you need to configure the incoming and outgoing channels. In the `src/main/resources/application.properties` configuration file, add the following configuration:
+
----
# Configuration file

quarkus.log.console.enable=true
quarkus.log.console.level=INFO
quarkus.log.level=INFO

# Configure the Kafka sources
mp.messaging.incoming.mission-command.connector=smallrye-kafka
mp.messaging.incoming.mission-command.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.mission-command.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.incoming.mission-command.request.timeout.ms=30000
mp.messaging.incoming.mission-command.enable.auto.commit=false

# Configure the Kafka sink
mp.messaging.outgoing.mission-event.connector=smallrye-kafka
mp.messaging.outgoing.mission-event.key.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.mission-event.value.serializer=org.apache.kafka.common.serialization.StringSerializer
mp.messaging.outgoing.mission-event.acks=1
----
+
* Both channels use the `smallrye-kafka` connector.
* The Kafka client uses string serialization and deserialization, for both keys and values.
* The `mission-command` channel has `enable.auto.commit` set to false. That's why you need to explicitly handle message acknowledgement in the code. You can also set the setting to `true` and remove the `@Acknowledgement` annotation from the code.
* The URL for the Kafka broker, as well as the name of the topics and the consumer group ID will be set at runtime, as these will typically be different depending on the environment.

=== Test the Processing Pipeline

Next step is to test the application. The application expects a running Kafka broker, which can be cumbersome to set up. Technologies like Docker or Test Containers make it relatively straightforward to spin up infrastructure components like a message broker or a distributed cache, but for your unit tests you probably want to mock this infrastructure.

SmallRye Reactive Messaging proposes an in-memory connector for this exact purpose. It allows switching the connector used for a channel with an in-memory connector. This in-memory connector provides a way to send messages to incoming channels, or check the received messages for outgoing channels without requiring any additional piece of infrastructure, which makes it ideal for unit tests.

. Add the following dependencies with test scope to the `pom.xml` file of the project:
+
----
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-junit5-mockito</artifactId>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>io.smallrye.reactive</groupId>
      <artifactId>smallrye-reactive-messaging-in-memory</artifactId>
      <version>2.3.0</version>
      <scope>test</scope>
    </dependency>
----

. Create a class `MissionCommandSourceTest` in the `com.redhat.emergency.response.mission.source` package of the `src/test/java` folder of the project.

. Annotate the test class with `@QuarkusTest`.

. Inject a mock instance of `MissionRepository` and `RoutePlanner` by using the `@InjectMock` Quarkus annotation. +
At this point your class should look like:
+
----
@QuarkusTest
public class MissionCommandSourceTest {

    @InjectMock
    RoutePlanner routePlanner;

    @InjectMock
    MissionRepository repository;
----

. Inject the SmallRye in-memory connector. Add a method to empty the connector sink before each test.
+
----
    @Inject @Any
    InMemoryConnector connector;

    @BeforeEach
    void init() {
        connector.sink("mission-event").clear();
    }
----

. Add a test method to test the processing of an incoming message. Annotate the method with the JUnit `@Test` annotation.
+
----
    @Test
    void testProcessMessage() {

    }
----

. Implement the test method. Your test case should:
* Set up the in-memory connector for the `mission-event` and `mission-command` channels.
* Set up the `RoutePlanner` mock.
* Use `InMemorySource.send()` to have a string payload sent to the source in-memory channel
* Verify that the sink in-memory channel received a message.
* Verify that the `RoutePlanner` and `MissionRepository` mocks have been called.
* A possible implementation looks like:
+
----
    @Test
    void testProcessMessage() {
        //Set up
        InMemorySink<String> missionEvents = connector.sink("mission-event");
        InMemorySource<String> missionCommand = connector.source("mission-command");

        MissionStep missionStep1 = new MissionStep();
        MissionStep missionStep2 = new MissionStep();

        Mockito.when(routePlanner.getDirections(Mockito.any(Location.class), Mockito.any(Location.class), Mockito.any(Location.class)))
                .thenReturn(Uni.createFrom().item(Arrays.asList(missionStep1, missionStep2)));

        // send message
        String payload = "{\"id\":\"91cf5e82-8135-476d-ade4-5fe00dca2cc6\",\"messageType\":\"CreateMissionCommand\","
                + "\"invokingService\":\"IncidentProcessService\",\"timestamp\":1593363522344,\"body\": "
                + "{\"incidentId\":\"incident123\",\"responderId\":\"responder123\",\"responderStartLat\":\"40.12345\","
                + "\"responderStartLong\":\"-80.98765\",\"incidentLat\":\"30.12345\",\"incidentLong\":\"-70.98765\","
                + "\"destinationLat\":\"50.12345\",\"destinationLong\":\"-90.98765\",\"processId\":\"0\"}}";

        missionCommand.send(payload);

        // verify
        MatcherAssert.assertThat(missionEvents.received().size(), Matchers.equalTo(1));

        Mockito.verify(routePlanner).getDirections(Mockito.any(Location.class), Mockito.any(Location.class), Mockito.any(Location.class));
        Mockito.verify(repository).put(Mockito.any(Mission.class));

    }
----

. Create a file `application.properties` in the `src/test/resources` folder. Add the following configuration to the file:
+
----
mp.messaging.outgoing.mission-event.connector=smallrye-in-memory
mp.messaging.incoming.mission-command.connector=smallrye-in-memory
----
+
When running a test class annotated with `@QuarkusTest`, the Quarkus test profile will be automatically activated, and the `src/test/resources/application.properties` will be used rather than the main application properties file.

. Run the test. From command line:
+
----
$ mvn clean test
----
+
Expect the test to succeed. If not, fix the code of the application or the test.

. To complete the unit test you can:
* Verify that the message contents received in the in-memory sink has the expected payload.
* Verify that the `RoutePlanner` and `MissionRepository` mocks have been called with the expected method parameters.
* Write tests for incoming message payloads which do not validate. In these tests verify that no message is received in the in-memory sink, and that the `RoutePlanner` and `MissionRepository` mocks are never called.

=== Deploy the Mission Service

Now you're ready to test the application with a real Kafka broker. 

. Make sure you are logged in the OpenShift cluster as a user with admin privileges.
. Check out the Ansible installer for the Emergency Response demo. Change directory to the `ansible` directory.
+
----
$ git clone https://github.com/gpte-cloud-native-advanced/erdemo-install.git
$ cd erdemo-install/ansible
----
. Copy the inventory template file:
+
----
$ cp inventories/inventory.template inventories/inventory
----

. Deploy the AMQ Streams operator:
+
----
$ ansible-playbook -i inventories/inventory playbooks/amq_streams_operator.yml -e project_admin=user1 -e project_name=reactive-messaging
----
+
This command deploys the AMQ Streams operator in the `user1-reactive-messaging` namespace. The scope of the operator is the namespace itself.

. Deploy the AMQ Streams cluster:
+
----
$ ansible-playbook -i inventories/inventory playbooks/kafka_cluster.yml -e project_admin=user1 -e project_name=reactive-messaging -e zookeeper_storage_type=ephemeral -e kafka_storage_type=ephemeral
----
+
This command deploys a Kafka cluster consisting of 3 ZooKeeper nodes and 3 Kafka broker nodes in the `user1-reactive-messaging` namespace. Both Zookeeper and the Kafka brokers use ephemeral storage - which is perfectly acceptable in a short-lived development environment. 

. Deploy the Kafka topics:
+
----
$ ansible-playbook -i inventories/inventory playbooks/kafka_topics.yml -e project_admin=user1 -e project_name=reactive-messaging
----
+
This command deploys the Kafka topics used by the Emergency Response application. Every topic is created with 15 partitions and a replication factor of 3.

. Deploy the Kafka producer app, a simple Quarkus application that exposes a REST endpoint and sends the payload of the REST call as a Kafka message to a given topic. You use this app to send messages to the `topic-mission-command` topic, which is the topic the mission service consumes _CreateMessageCommand_ messages from. +
Deploy the Kafka producer application:
+
----
$ ansible-playbook -i inventories/inventory playbooks/kafka_producer_app.yml -e project_admin=user1 -e project_name=reactive-messaging -e kafka_topic=topic-mission-command
----

. Deploy the Kafka consumer application. The Kafka consumer application is a simple Quarkus application that consumes messages from a given topic and logs the payload and metadata of each message to _stdout_. You use this application to verify the _MissionCreatedEvent_ messages produced by the mission service.
+
----
$ ansible-playbook -i inventories/inventory playbooks/kafka_consumer_app.yml -e project_admin=user1 -e project_name=reactive-messaging -e kafka_topic=topic-mission-event
----

. Create a file called `application.properties` to hold the externalized configuration for the mission service.
+
----
echo '
quarkus.log.console.level=DEBUG
quarkus.log.category."com.redhat.emergency.response.mission".level=DEBUG

kafka.bootstrap.servers=kafka-cluster-kafka-bootstrap.user1-reactive-messaging.svc:9092

mp.messaging.incoming.mission-command.topic=topic-mission-command
mp.messaging.incoming.mission-command.group.id=mission-service

mp.messaging.outgoing.mission-event.topic=topic-mission-event
' | tee /tmp/application.properties
----

. Create the configmap for the mission service:
+
----
$ oc create configmap mission-service --from-file=/tmp/application.properties -n user1-reactive-messaging
----

. Create a deploymentconfig for the mission service application:
+
----
$ echo '
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  labels:
    app: mission-service
  name: mission-service
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    app: mission-service
    group: erd-services
  strategy:
    activeDeadlineSeconds: 21600
    resources: {}
    rollingParams:
      intervalSeconds: 1
      maxSurge: 25%
      maxUnavailable: 25%
      timeoutSeconds: 3600
      updatePeriodSeconds: 1
    type: Rolling
  template:
    metadata:
      labels:
        app: mission-service
        group: erd-services
    spec:
      containers:
      - env:
        - name: KUBERNETES_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: mission-service
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 3
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 250m
            memory: 250Mi
          requests:
            cpu: 100m
            memory: 100Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /deployments/config
          name: config
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          name: mission-service
        name: config
  triggers:
  - type: ConfigChange
  - imageChangeParams:
      automatic: true
      containerNames:
      - mission-service
      from:
        kind: ImageStreamTag
        name: mission-service:latest
    type: ImageChange
' | oc create -f - -n user1-reactive-messaging
----

. Build the mission service application, create an image and push to the OpenShift cluster:
+
----
$ mvn clean package
$ REGISTRY_URL=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')
$ podman build -f docker/Dockerfile -t ${REGISTRY_URL}/user1-reactive-messaging/mission-service:latest .
$ podman login -u $(oc whoami) -p $(oc whoami -t) ${REGISTRY_URL}
$ podman push ${REGISTRY_URL}/user1-reactive-messaging/mission-service:latest
----
+
This triggers a deployment of the mission service.

. Send a _CreateMissionCommand_ message to the mission service through the Kafka producer app:
+
----
$ echo '
{
    "key":"829fce70-83ae-49dd-b0dc-6dfbdfd7dc43",
    "value":{
      "messageType" : "CreateMissionCommand",
      "id":"4a6d9a14-5de6-4f04-9125-85819c4824b0",
      "invokingService":"test",
      "timestamp":1521148332397,
      "body": {
        "incidentId": "829fce70-83ae-49dd-b0dc-6dfbdfd7dc43",
        "responderId": "responder123",
        "responderStartLat": "31.12345",
        "responderStartLong": "-71.98765",
        "incidentLat": "30.12345",
        "incidentLong": "-70.98765",
        "destinationLat": "32.12345",
        "destinationLong": "-72.98765",
        "processId": "1"
    }
  }
}
' | tee /tmp/create-mission.json
$ KAFKA_PRODUCER_APP=http://$(oc get route kafka-producer-app -n user1-reactive-messaging --template='{{ .spec.host }}')
$ curl -v -X POST -H "Content-type: application/json" -d @/tmp/create-mission.json ${KAFKA_PRODUCER_APP}/produce
----

. Check the logs of the Kafka consumer application pod. Expect to see a log statement of the _MissionStartedEvent_ message produced by the mission service.
+
----
2020-07-14 10:14:01,997 INFO  [com.red.eme.res.kaf.KafkaRecordConsumer] (Thread-4) Consumed message from topic 'topic-mission-event', partition '11', offset '0'
2020-07-14 10:14:01,999 INFO  [com.red.eme.res.kaf.KafkaRecordConsumer] (Thread-4)     Headers: 
2020-07-14 10:14:01,999 INFO  [com.red.eme.res.kaf.KafkaRecordConsumer] (Thread-4)     Message key: 829fce70-83ae-49dd-b0dc-6dfbdfd7dc43
2020-07-14 10:14:01,999 INFO  [com.red.eme.res.kaf.KafkaRecordConsumer] (Thread-4)     Message value: {"id":"26528ca0-31dd-406b-a5ca-cb5feb42555b","invokingService":"MissionService","timestamp":1594721641574,"messageType":"MissionStartedEvent","body":{"id":"e78b2b5b-dbbf-44d3-9f9f-a1615c62f938","incidentId":"829fce70-83ae-49dd-b0dc-6dfbdfd7dc43","responderId":"responder123","responderStartLat":31.12345,"responderStartLong":-71.98765,"incidentLat":30.12345,"incidentLong":-70.98765,"destinationLat":32.12345,"destinationLong":-72.98765,"responderLocationHistory":[],"status":"CREATED","steps":[]}}
----

. Check the logs of the mission service. Expect to see a log statement with the contents of the incoming message.
+
----
2020-07-14 10:20:32,607 DEBUG [com.red.eme.res.mis.sou.MissionCommandSource] (vert.x-eventloop-thread-0) Consumed message from topic 'topic-mission-command', partition:offset '11:2'
2020-07-14 10:20:32,609 DEBUG [com.red.eme.res.mis.sou.MissionCommandSource] (vert.x-eventloop-thread-0) Processing message: {"messageType":"CreateMissionCommand","id":"4a6d9a14-5de6-4f04-9125-85819c4824b0","invokingService":"test","timestamp":1521148332397,"body":{"incidentId":"829fce70-83ae-49dd-b0dc-6dfbdfd7dc43","responderId":"responder123","responderStartLat":"31.12345","responderStartLong":"-71.98765","incidentLat":"30.12345","incidentLong":"-70.98765","destinationLat":"32.12345","destinationLong":"-72.98765","processId":"1"}}
----
+
Notice that the processing of the message is done on an event loop thread. When developing reactive applications it is essential not to block the event loop threads. So if message processing uses blocking operations - like calling an external service - you need to make sure that these operations are executed asynchronously on a worker thread. This is how the `RoutePlanner` service is modeled in the mission service.

== Tear down the test environment

In order to free up resources on the OpenShift cluster, you can tear down the environment you deployed to test out the mission-service. +
To do so, execute the following Ansible commands:

----
$ ansible-playbook -i inventories/inventory playbooks/kafka_producer_app.yml -e project_admin=user1 -e project_name=reactive-messaging -e ACTION=uninstall 
$ ansible-playbook -i inventories/inventory playbooks/kafka_consumer_app.yml -e project_admin=user1 -e project_name=reactive-messaging -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/mission_service.yml -e project_admin=user1 -e project_name=reactive-messaging -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/kafka_topics.yml -e project_admin=user1 -e project_name=reactive-messaging -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/kafka_cluster.yml -e project_admin=user1 -e project_name=reactive-messaging -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/amq_streams_operator.yml -e project_admin=user1 -e project_name=reactive-messaging -e ACTION=uninstall
----