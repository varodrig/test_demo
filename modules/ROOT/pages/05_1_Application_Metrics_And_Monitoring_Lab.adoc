:noaudio:
:scrollbar:
:toc2:
:linkattrs:
:data-uri:

== Application Metrics and Monitoring Lab

.Prerequisites
* Access to a Red Hat^(R)^ OpenShift^(R)^ Container Platform 4.x cluster
* Prerequisites for local workstation:
** Java development environment for Java 11, including an IDE
** Development tools: git, Maven (version 3.6.3)
** Tools for container image management (Docker, Podman)
** API Testing tools (Postman, SoapUI)
** Ansible (version >= 2.9.0)
** Openshift `oc` CLI client, version 4.6.x

.Goals
* Expose white box and black box metrics in Quarkus applications
* Collect metrics with Prometheus and visualize with Grafana

:numbered:

== Introduction

Application performance monitoring is essential to be able to assert that your applications work and perform as expected and deliver the expected business value.

There are numerous tools and products on the market that provide monitoring capabilities at infrastructure and application level, both open-source and proprietary.

Prometheus (https://prometheus.io) is rapidly gaining traction as the open-source monitoring tool for cloud-native applications. Prometheus is integrated into OpenShift to provide cluster-wide monitoring capabilities at the infrastructure level, but it is equally well suited for application-level monitoring.

The central component of Prometheus is the _Prometheus server_. The Prometheus server scrapes targets at a configurable interval to collect metrics from specific targets and store them in a time-series database. Targets--the systems or applications that need to be monitored-- typically expose an HTTP endpoint providing metrics. Prometheus has a wide range of service discovery options to find the target services and start retrieving metrics from them, including integration with OpenShift/Kubernetes.

The data gathered and stored by the Prometheus server can be queried using the _PromQL_ language. The Prometheus UI has some limited capacities to show graphs from the collected metrics. Prometheus is often used together with _Grafana_ (https://grafana.com) to provide dashboards on top of the metrics collected by Prometheus.

This diagram illustrates the architecture of Prometheus and some of its ecosystem components:

image::images/prometheus-architecture.svg[]

Application need to be instrumented to provide relevant metrics. Quarkus implements the MicroProfile Metrics specification, which allows to easily instrument application code with annotations to produce application specific metrics. The Quarkus runtime itself and some of the Quarkus extensions also produce metrics using MicroProfile Metrics.

MicroProfile Metrics metrics are exported in JSON or OpenMetrics format. OpenMetrics is compatible with the Prometheus format, so metrics produced by a MicroProfile Metrics implementation can be ingested by Prometheus without further transformation.

== Instrument Quarkus applications to expose metrics

=== Deploy a Development Environment

In this lab you will instrument the incident service of the Emergency Response application to expose built-in and custom metrics.

The first step consists of creating a development environment on OpenShift. +
The incident service requires a AMQ Streams cluster as well as a PostgreSQL database. 

. Make sure you are logged in the OpenShift cluster as a user with admin privileges.
. Check out the Ansible installer for the Emergency Response application. Change directory to the `ansible` directory.
+
----
$ git clone https://github.com/gpte-cloud-native-advanced/erdemo-install.git
$ cd erdemo-install/ansible
----
. Copy the inventory template file
+
----
$ cp inventories/inventory.template inventories/inventory
----

. Deploy the AMQ Streams operator:
+
----
$ ansible-playbook -i inventories/inventory playbooks/amq_streams_operator.yml -e project_admin=user1 -e project_name=metrics
----
+
This command deploys the AMQ Streams operator in the `user1-metrics` namespace. The scope of the operator is the namespace itself.

. Deploy the AMQ Streams cluster:
+
----
$ ansible-playbook -i inventories/inventory playbooks/kafka_cluster.yml -e project_admin=user1 -e project_name=metrics -e zookeeper_storage_type=ephemeral -e kafka_storage_type=ephemeral
----
+
This command deploys a Kafka cluster consisting of 3 ZooKeeper nodes and 3 Kafka broker nodes in the `user1-metrics` namespace. Both Zookeeper and the Kafka brokers use ephemeral storage - which is perfectly acceptable in a short-lived development environment. 

. Deploy the Kafka topics:
+
----
$ ansible-playbook -i inventories/inventory playbooks/kafka_topics.yml -e project_admin=user1 -e project_name=metrics
----
+
This command deploys the Kafka topics used by the Emergency Response application. Every topic is created with 15 partitions and a replication factor of 3.

. Deploy the PostgreSQL database. 
+
----
$ ansible-playbook -i inventories/inventory playbooks/postgresql.yml -e project_admin=user1 -e project_name=metrics -e postgresql_storage_type=ephemeral
----
+
This command deploys a PostgreSQL version 12 database. +
As part of the deployment of the PostgreSQL instance, the Emergency Response database and tables are created using deployment pod-based lifecycle hooks.

. Deploy the incident service
+
----
$ ansible-playbook -i inventories/inventory playbooks/incident_service.yml -e project_admin=user1 -e project_name=metrics -e expose_service=true
----
+
This command deploys the incident service image and configures the application configuration configmap. The incident service is exposed through a route.

=== Instrument the Incident Service

Several Quarkus extensions can be configured to expose metrics over a metric endpoint. Metrics can be exposed in JSON format, or in OpenMetrics format, which is compatible with Prometheus.

. Check the metrics endpoint on the incident service. Obtain a remote shell to the incident service pod and query the endpoint using `curl`.
+
----
$ oc project user1-metrics
$ INCIDENT_SERVICE_POD=$(oc get pods -o name | grep incident-service-[0-9]*-[^deploy])
$ oc rsh $INCIDENT_SERVICE_POD
sh-4.4$ curl http://localhost:8080/metrics
----
+
.Sample Output
----
RESTEASY003210: Could not find resource for full path: http://127.0.0.1:8080/metrics
----
+
Notice that by default, no metrics are exposed by the application. The application has to be explicitly configured to enable metrics.

. Check out the source code for the incident service:
+
----
$ git clone https://github.com/gpte-cloud-native-advanced/incident-service.git
$ cd incident-service
----
. Import the code into your IDE of choice.
. Ensure the code builds correctly and the unit tests are succeeding:
+
----
$ mvn clean package
----

. Quarkus follows the MicroProfile Metrics specification, which is implemented by the SmallRye Metrics project (https://smallrye.io/docs/smallrye-metrics/2.4.0/index.html). +
Add the `smallrye-metrics` Quarkus extension to the `pom.xml` file of the project:
+
----
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-smallrye-metrics</artifactId>
    </dependency>
----

. Build the application, create an application image and deploy the image to the OpenShift registry.
+
----
$ mvn clean package
$ REGISTRY_URL=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')
$ podman build -f docker/Dockerfile -t ${REGISTRY_URL}/user1-metrics/incident-service:metrics .
$ podman login -u $(oc whoami) -p $(oc whoami -t) ${REGISTRY_URL}
$ podman push ${REGISTRY_URL}/user1-metrics/incident-service:metrics
----

. Patch the deploymentconfig of the incident service to point to the new image. This will force a redeployment of the incident service application. 
+
----
$ oc patch dc incident-service --type='json' -p '[{"op": "replace", "path": "/spec/triggers/1/imageChangeParams/from/name", "value": "incident-service:metrics"}]' -n user1-metrics
----

. Using curl, verify the metrics endpoint of the incident service pod:
+
----
$ oc project user1-metrics
$ INCIDENT_SERVICE_POD=$(oc get pods -o name | grep incident-service-[0-9]*-[^deploy])
$ oc rsh $INCIDENT_SERVICE_POD
sh-4.4$ curl http://localhost:8080/metrics
----
+
.Sample Output
----
# HELP base_gc_time_total Displays the approximate accumulated collection elapsed time in milliseconds. This attribute displays -1 if the collection elapsed time is undefined for this collector. The Java virtual machine implementation may use a high resolution timer to measure the elapsed time. This attribute may display the same value even if the collection count has been incremented if the collection elapsed time is very short.
# TYPE base_gc_time_total counter
base_gc_time_total_seconds{name="PS MarkSweep"} 0.529
# HELP base_cpu_processCpuLoad_percent Displays  the "recent cpu usage" for the Java Virtual Machine process. This value is a double in the [0.0,1.0] interval. A value of 0.0 means that none of the CPUs were running threads from the JVM process during the recent period of time observed, while a value of 1.0 means that all CPUs were actively running threads from the JVM 100% of the time during the recent period being observed. Threads from the JVM include the application threads as well as the JVM internal threads. All values betweens 0.0 and 1.0 are possible depending of the activities going on in the JVM process and the whole system. If the Java Virtual Machine recent CPU usage is not available, the method returns a negative value.
# TYPE base_cpu_processCpuLoad_percent gauge
base_cpu_processCpuLoad_percent 2.71957055713902E-7
# HELP base_memory_maxHeap_bytes Displays the maximum amount of heap memory in bytes that can be used for memory management. This attribute displays -1 if the maximum heap memory size is undefined. This amount of memory is not guaranteed to be available for memory management if it is greater than the amount of committed memory. The Java virtual machine may fail to allocate memory even if the amount of used memory does not exceed this maximum size.
# TYPE base_memory_maxHeap_bytes gauge
base_memory_maxHeap_bytes 2.3330816E8
# HELP base_cpu_systemLoadAverage Displays the system load average for the last minute. The system load average is the sum of the number of runnable entities queued to the available processors and the number of runnable entities running on the available processors averaged over a period of time. The way in which the load average is calculated is operating system specific but is typically a damped time-dependent average. If the load average is not available, a negative value is displayed. This attribute is designed to provide a hint about the system load and may be queried frequently. The load average may be unavailable on some platforms where it is expensive to implement this method.
# TYPE base_cpu_systemLoadAverage gauge
base_cpu_systemLoadAverage 1.06
# HELP base_memory_committedHeap_bytes Displays the amount of memory in bytes that is committed for the Java virtual machine to use. This amount of memory is guaranteed for the Java virtual machine to use.
# TYPE base_memory_committedHeap_bytes gauge
base_memory_committedHeap_bytes 1.9398656E7
# HELP base_gc_total Displays the total number of collections that have occurred. This attribute lists -1 if the collection count is undefined for this collector.
# TYPE base_gc_total counter
base_gc_total{name="PS MarkSweep"} 4.0
# HELP base_thread_daemon_count Displays the current number of live daemon threads.
# TYPE base_thread_daemon_count gauge
base_thread_daemon_count 10.0
# HELP base_cpu_availableProcessors Displays the number of processors available to the Java virtual machine. This value may change during a particular invocation of the virtual machine.
# TYPE base_cpu_availableProcessors gauge
base_cpu_availableProcessors 1.0
# HELP base_thread_max_count Displays the peak live thread count since the Java virtual machine started or peak was reset. This includes daemon and non-daemon threads.
# TYPE base_thread_max_count gauge
base_thread_max_count 21.0
# HELP base_memory_usedHeap_bytes Displays the amount of used heap memory in bytes.
# TYPE base_memory_usedHeap_bytes gauge
base_memory_usedHeap_bytes 1.7828784E7
base_gc_total{name="PS Scavenge"} 146.0
# HELP base_classloader_loadedClasses_count Displays the number of classes that are currently loaded in the Java virtual machine.
# TYPE base_classloader_loadedClasses_count gauge
base_classloader_loadedClasses_count 9007.0
# HELP base_thread_count Displays the current number of live threads including both daemon and non-daemon threads
# TYPE base_thread_count gauge
base_thread_count 21.0
base_gc_time_total_seconds{name="PS Scavenge"} 0.874
# HELP base_classloader_loadedClasses_total Displays the total number of classes that have been loaded since the Java virtual machine has started execution.
# TYPE base_classloader_loadedClasses_total counter
base_classloader_loadedClasses_total 9007.0
# HELP base_classloader_unloadedClasses_total Displays the total number of classes unloaded since the Java virtual machine has started execution.
# TYPE base_classloader_unloadedClasses_total counter
base_classloader_unloadedClasses_total 0.0
# HELP base_jvm_uptime_seconds Displays the time from the start of the Java virtual machine in milliseconds.
# TYPE base_jvm_uptime_seconds gauge
base_jvm_uptime_seconds 47.153
# HELP vendor_memoryPool_usage_max_bytes Peak usage of the memory pool denoted by the 'name' tag
# TYPE vendor_memoryPool_usage_max_bytes gauge
vendor_memoryPool_usage_max_bytes{name="PS Old Gen"} 1.5451472E7
# HELP vendor_memory_committedNonHeap_bytes Displays the amount of non heap memory in bytes that is committed for the Java virtual machine to use.
# TYPE vendor_memory_committedNonHeap_bytes gauge
vendor_memory_committedNonHeap_bytes 6.586368E7
# HELP vendor_memoryPool_usage_bytes Current usage of the memory pool denoted by the 'name' tag
# TYPE vendor_memoryPool_usage_bytes gauge
vendor_memoryPool_usage_bytes{name="CodeHeap 'profiled nmethods'"} 6713600.0
vendor_memoryPool_usage_max_bytes{name="PS Survivor Space"} 1015872.0
# HELP vendor_cpu_systemCpuLoad_percent Displays the "recent cpu usage" for the whole system. This value is a double in the [0.0,1.0] interval. A value of 0.0 means that all CPUs were idle during the recent period of time observed, while a value of 1.0 means that all CPUs were actively running 100% of the time during the recent period being observed. All values betweens 0.0 and 1.0 are possible depending of the activities going on in the system. If the system recent cpu usage is not available, the method returns a negative value.
# TYPE vendor_cpu_systemCpuLoad_percent gauge
vendor_cpu_systemCpuLoad_percent 0.11891424384229066
vendor_memoryPool_usage_bytes{name="PS Old Gen"} 1.2392016E7
vendor_memoryPool_usage_bytes{name="PS Survivor Space"} 294912.0
vendor_memoryPool_usage_bytes{name="Compressed Class Space"} 5750728.0
vendor_memoryPool_usage_max_bytes{name="CodeHeap 'non-nmethods'"} 1355520.0
vendor_memoryPool_usage_max_bytes{name="Metaspace"} 4.6274592E7
vendor_memoryPool_usage_max_bytes{name="CodeHeap 'non-profiled nmethods'"} 1351424.0
# HELP vendor_memory_maxNonHeap_bytes Displays the maximum amount of used non-heap memory in bytes.
# TYPE vendor_memory_maxNonHeap_bytes gauge
vendor_memory_maxNonHeap_bytes -1.0
vendor_memoryPool_usage_bytes{name="Metaspace"} 4.6274752E7
vendor_memoryPool_usage_max_bytes{name="PS Eden Space"} 3145728.0
vendor_memoryPool_usage_max_bytes{name="CodeHeap 'profiled nmethods'"} 6713600.0
# HELP vendor_memory_freePhysicalSize_bytes Displays the amount of free physical memory in bytes.
# TYPE vendor_memory_freePhysicalSize_bytes gauge
vendor_memory_freePhysicalSize_bytes 4.668506112E9
vendor_memoryPool_usage_bytes{name="PS Eden Space"} 0.0
vendor_memoryPool_usage_bytes{name="CodeHeap 'non-profiled nmethods'"} 1351424.0
# HELP vendor_memory_freeSwapSize_bytes Displays the amount of free swap space in bytes.
# TYPE vendor_memory_freeSwapSize_bytes gauge
vendor_memory_freeSwapSize_bytes 4.668506112E9
vendor_memoryPool_usage_bytes{name="CodeHeap 'non-nmethods'"} 1306752.0
# HELP vendor_cpu_processCpuTime_seconds Displays the CPU time used by the process on which the Java virtual machine is running in nanoseconds. The returned value is of nanoseconds precision but not necessarily nanoseconds accuracy. This method returns -1 if the the platform does not support this operation.
# TYPE vendor_cpu_processCpuTime_seconds gauge
vendor_cpu_processCpuTime_seconds 7.87
vendor_memoryPool_usage_max_bytes{name="Compressed Class Space"} 5750728.0
# HELP vendor_memory_usedNonHeap_bytes Displays the amount of used non-heap memory in bytes.
# TYPE vendor_memory_usedNonHeap_bytes gauge
vendor_memory_usedNonHeap_bytes 6.1419144E7
----
+
* Adding the Quarkus `smallrye-metrics` configures the application to expose metrics about the JVM CPU usage, memory usage and thread usage.
* As mandated by the MicroProfile specification the metrics are divided into scopes: base metrics, vendor-specific metrics and application metrics.
* Base metrics are required metrics that every MicroProfile implementation has to expose. They include JVM stats and operation system stats, like the number of available CPU.

. Several Quarkus extensions expose their owns set of metrics, if enabled. For the incident service this includes the _RESTEasy_ extension, which provides REST endpoint capabilities to applications, as well as the _Datasource_ extension, which provides stats around the datasource connection pools. 

. To enable metrics on the REST extension, add the following to the `application.properties` configuration file in the `src/main/resources` folder:
+
----
quarkus.resteasy.metrics.enabled=true
----
+
The RESTEasy extensions produces metrics on REST requests to the applications. More specifically it keeps track of the number of invocations, and the total response time per REST endpoint.

. To enable metrics on the Datasource extension (`quarkus-agroal`), add the following to the `application.properties` file:
+
----
quarkus.datasource.metrics.enabled=true
----
+
The datasource extension exposes metrics on the underlying datasource connection pool (powered by _Agroal_), such as the number of active connections in the pool, the number of available connections in the pool, the maximum connections used etc.
+
NOTE: The incident service also produces and consumes messages to and from a Kafka cluster. However, the underlying Kafka client only exposes metrics through JMX. To make these metrics available for scraping by Prometheus, you would need to install the Prometheus JMX Exporter and run it as a Java Agent on top of the JVM. This is outside of the scope of this lab. On the other hand, you will see how you can use Kafka server-side metrics to gain useful information about the behavior of the incident service client application.

. In a lot of cases, relying only on default metrics exposed by subsystems and frameworks used by the application will not be enough to gain full insight in how the application behaves in a production environment. +
Additional metrics can be produced by explicit instrumentation of the application code. +
The MicroProfile Metrics specification defines a number of annotations for that purpose. These annotations are applied to methods (or classes) to gather metrics when the methods are invoked. 
* Specifically for the incident service, you may be interested to have metrics about latency and throughput of persistence operations. +
The two persistence operations that are called by other services in the Emergency Response application are _create_ and _update_. +
Add the `SimplyTimed` annotation to the `create` and `update` methods in the `com.redhat.emergency.response.incident.service.IncidentService` class.
+
----
    @SimplyTimed(name = "IncidentService.repository", tags = {"operation=create"}, absolute = true)
    @Transactional
    public JsonObject create(JsonObject incident) {
        Incident created = repository.create(toEntity(incident));

        return fromEntity(created);
    }

    @SimplyTimed(name = "IncidentService.repository", tags = {"operation=update"}, absolute = true)
    @Transactional
    public JsonObject updateIncident(JsonObject incident) {
        Incident current = repository.findByIncidentId(incident.getString("id"));
        [...]
        return fromEntity(current);
    }
----
+
* The `name` attribute is the name of the timer, and will also be the name of the metrics produced. The `tags` attribute allow to further subdivide the metrics. The `absolute` attribute determines if the name should be pre-fixed with the class name. If true, the name will not be pre-fixed.
* `SimpleTimer` is only one of the metric types defined by the MicroProfile specification. Others are `Gauge`, `Counter`, `Histogram`, and `Timer`.
* `SimpleTimer` produces two metrics: the total count of invocations of the annotated method, and the total time spent invoking the method. These metrics allow to calculate throughput (how many invocations per time unit) and latency (how long does an invocation take).
* `Timer` is slightly more sophisticated, in the sense that it will also keep track of running averages and percentiles. However, running averages can also be calculated on the Prometheus server, so in order to keep the timer as lightweight and non-intrusive as possible, SimpleTimer is most of the time sufficient.
* `Histograms` and `Timers` are the most elaborate kind of timers, which also calculate the distribution of a value. Histograms are useful when you for example need to verify SLAs (for example, 95% of invocations must be executed within 200ms).

. The `create` and `update` methods are also annotated with `@Transactional`. That means that at build time, Quarkus will weave a transaction handling interceptor around the method. The `SimpleTimer` annotation will also produce an interceptor. There is no guarantee about the order of interceptors so if you want to include the transaction handling into the metered invocation, the code needs to be reorganized:
+
----
    @SimplyTimed(name = "IncidentService.repository", tags = {"operation=create"}, absolute = true)
    public JsonObject create(JsonObject incident) {
        return doCreate(incident);
    }

    @Transactional
    JsonObject doCreate(JsonObject incident) {
        Incident created = repository.create(toEntity(incident));
        return fromEntity(created);
    }

    @SimplyTimed(name = "IncidentService.repository", tags = {"operation=update"}, absolute = true)
    public JsonObject updateIncident(JsonObject incident) {
        return doUpdate(incident);
    }

    @Transactional
    JsonObject doUpdate(JsonObject incident) {
        Incident current = repository.findByIncidentId(incident.getString("id"));
        [...]
        return fromEntity(current);
    }
----

. Build an application image and deploy the image to the OpenShift registry.
+
----
$ mvn clean package
$ REGISTRY_URL=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')
$ podman build -f docker/Dockerfile -t ${REGISTRY_URL}/user1-metrics/incident-service:metrics .
$ podman login -u $(oc whoami) -p $(oc whoami -t) ${REGISTRY_URL}
$ podman push ${REGISTRY_URL}/user1-metrics/incident-service:metrics
----

== Collect Metrics with Prometheus and Visualize with Grafana

=== Deploy the Monitoring Stack

Now that the application code has been instrumented, and the new code deployed in the development environment, you can deploy a monitoring stack including Prometheus and Grafana to collect the metrics and build a monitoring dashboard. 

. Deploy the monitoring stack. +
In a terminal window, navigate to the directory where you checked out the Ansible installer for the Emergency Response application. Change to the `ansible` directory, and run the following Ansible playbook:
+
----
$ ansible-playbook -i inventories/inventory playbooks/monitoring.yml -e project_admin=user1 -e project_name=metrics -e namespace_monitoring=user1-metrics -e monitoring_label_value=user1-metrics
----
+
This playbook deploys a monitoring stack consisting of an instance of Prometheus and Grafana. It also deploys a `GrafanaDatasource` instance to allow Grafana to obtain metrics data from the Prometheus instance. 

. Wait until the Prometheus and Grafana pods are up and running
+
----
$ oc get pods -n user1-metrics | grep -E "grafana-deployment-.*|prometheus-user1-monitoring.*"
----
+
.Sample Output
----
grafana-deployment-7bbfdf5c7c-nvptx                   2/2     Running     0          4m19s
prometheus-user1-monitoring-0                         4/4     Running     1          4m56s
----

=== Collect Metrics in Prometheus

. Obtain the URL to the Prometheus application:
+
----
$ PROMETHEUS_URL=https://$(oc get route prometheus-route -n user1-metrics --template='{{ .spec.host }}')
$ echo ${PROMETHEUS_URL}
----

. In a browser window, navigate to the Prometheus application. Log in with your OpenShift admin credentials. Authorize access by clicking the _Allow selected permissions_ button. Expect to see the Prometheus home page:
+
image::images/prometheus-home-page.png[]

. In the Prometheus app, navigate to `Status -> Targets`. This page lists the scraping targets discovered by Prometheus. Notice that the incident service does not appear in this list.

. The service account with which the Prometheus pod is running needs sufficient privileges to get and list pods, services and endpoints.
* Create a role in the `user1-monitoring` namespace which defines the required privileges:
+
----
$ echo '
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: prometheus-monitoring
rules:
- apiGroups: [""]
  resources:
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
' | oc create -f - -n user1-metrics
----
* Add the role to the Prometheus service account:
+
----
$ oc adm policy add-role-to-user prometheus-monitoring system:serviceaccount:user1-metrics:prometheus-service-account --role-namespace=user1-metrics
----

. The way Prometheus discovers targets depends on the environment and how Prometheus is configured. When installed though the Prometheus operator, discovery targets are configured through ServiceMonitor custom resources. +
Create a _ServiceMonitor_ custom resource for the incident service application:
+
---- 
$ echo '
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: incident-service
spec:
  endpoints:
    - interval: 5s
      path: /metrics
      port: http
  namespaceSelector:
    matchNames:
      - user1-metrics
  selector:
    matchLabels:
      monitoring: prometheus
' | oc create -f - -n user1-metrics
----
+
* The `namespaceSelector` defines the target namespace(s) for discovery.
* The `selector` defines the labels that must be present on the services, endpoints or pods that need to be discovered.
* The endpoints define the scraping details for Prometheus. In this case Prometheus will scrape the `/metrics` endpoint on the port named `http`, every 5 seconds.

. For the incident service to be discovered, the service object needs to be labeled with the correct label:
+
----
$ oc label service incident-service -n user1-metrics monitoring=prometheus
----

. Check on the target page of Prometheus that the incident service is discovered. This make take a couple of seconds, and you need to refresh the page to see any changes appearing. Expect to see the incident service appearing in the list of discovered targets:
+
image::images/prometheus-targets-incident-service.png[]

. Go back to the home page of Prometheus. Click on the drop-down box next to the `Execute` button to see the list of metrics scraped by Prometheus. Look for metrics prefixed with `base`, `application` and `vendor`. These are the metrics exposed by the incident service.

. Some metrics are created lazily, meaning they will only appear when there are actually metrics to report. This is the case for the REST metrics of the incident service. +
Create an incident by calling the REST API of the incident service.
+
----
$ echo '
{
  "lat": 34.98125,
  "lon": -77.84121,
  "numberOfPeople": 5,
  "medicalNeeded": true,
  "victimName": "Jane Foo",
  "victimPhoneNumber": "(458) 741-45823)"
}
' | tee /tmp/incident.json
$ INCIDENT_SERVICE_URL=$(oc get route incident-service -n user1-metrics --template='{{ .spec.host }}')
$ curl -v -X POST -H "Content-type: application/json" -d @/tmp/incident.json http://${INCIDENT_SERVICE_URL}/incidents
----
+
Repeat the `curl` command a couple of times.

. In Prometheus, select the `base_REST_request_elapsedTime_seconds` metric. You might need to refresh the page in order for the metric to appear in the drop-down box. Click the `Execute` button.
+
image::images/prometheus-incident-service-metrics.png[]
+
Notice the tags on the metric name. In this case for example, one of the tags is `method`, which represents the REST endpoint that was called. Tags allow to filter and group metrics.

. The REST metrics exposed by the Quarkus RestEasy extension, as well as the methods you annotated explicitly in the code are of type `SimpleTimer`. The metrics produced by a `SimpleTimer` are not that useful on their own, but they do allow to calculate throughput and latency. Prometheus offers a rich DSL called PromQL (https://prometheus.io/docs/prometheus/latest/querying/functions) to do statistical manipulations on raw metric data. +
For example, the PromQL `rate` function calculates the per-second average rate of increase of a time series. +
Using the `rate` we can calculate the throughput and latency for REST calls to the incident service. +
The throughput can be expressed as: `rate(base_REST_request_total[1m])`, which yields the per-second rate of REST requests as measured over the last minute.
Similarly, the latency can be expressed as `rate(base_REST_request_elapsedTime_seconds[1m])/rate(base_REST_request_total[1m])`. This yields the average duration of requests as measured in the last minute.

. Calculations as the ones shown above can be done on the fly, but they can also be pre-calculated and stored as a new time series in Prometheus. These predefined calculations are stored into Prometheus rule files. A Prometheus rule has a name and an expression. The name (or _record_) is the name of the time series being created. +
Prometheus rule files can be defined in a _PrometheusRule_ custom resource. +
Create a PrometheusRule custom resource for the incident service.
+
----
$ echo "
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: incident-service-prometheus-rules
spec:
  groups:
    - name: incident-service.rules
      rules:
        - record: incident_service:rest:elapsed_time:rate1m
          expr: rate(base_REST_request_elapsedTime_seconds{job=\"incident-service\"}[1m])
        - record: incident_service:rest:total:rate1m
          expr: rate(base_REST_request_total{job=\"incident-service\"}[1m])
        - record: incident_service:repository:elapsed_time:rate1m
          expr: rate(application_IncidentService_repository_elapsedTime_seconds{job=\"incident-service\"}[1m])
        - record: incident_service:repository:total:rate1m
          expr: rate(application_IncidentService_repository_total{job=\"incident-service\"}[1m])
" | oc create -f - -n user1-metrics
----

. In the Prometheus UI, navigate to `Status -> Rules`. Expect to see the rules you defined previously for the incident service. It can take a couple of seconds before the rules appear. Refresh the page to see changes appear.
+
image::images/prometheus-rules-incident-service.png[]

=== Build a Monitoring Dashboard in Grafana

Now that you have set up collection and handling of the metrics, you can build a dashboard in Grafana. Before doing so, you first add a datasource to Grafana to be able to get metric data from the OpenShift Prometheus instance.

The OpenShift Prometheus powers the usage graphs in the OpenShift UI. 

image::images/openshift-prometheus-graph.png[]

The metrics collected by the OpenShift Prometheus instance can also be used in custom application dashboards.

When installing the monitoring stack using the Ansible installation playbook, Grafana is deployed with a datasource pointing to the Prometheus instance of the stack. You can define additional Grafana datasources by deploying a _GrafanaDatasource_ custom resource.

. Create a Grafana datasource for the OpenShift Prometheus instance. The Prometheus instance requires authentication, so you first need to obtain a authentication token for the Grafana service account.
+
----
$ AUTH_TOKEN=$(oc sa get-token grafana-serviceaccount -n user1-metrics)
----

. Create the Grafana datasource custom resource:
+
----
$ echo "
apiVersion: integreatly.org/v1alpha1
kind: GrafanaDataSource
metadata:
  name: prometheus-openshift-monitoring
spec:
  datasources:
    - access: proxy
      editable: true
      jsonData:
        httpHeaderName1: Authorization
        timeInterval: 5s
        tlsSkipVerify: true
      name: OpenShift Monitoring
      secureJsonData:
        httpHeaderValue1: Bearer ${AUTH_TOKEN}
      type: prometheus
      url: 'https://prometheus-k8s.openshift-monitoring.svc:9091'
  name: openshift-monitoring.yaml
" |  oc create -f - -n user1-metrics
----
+
This will cause the Grafana pod to redeploy. Wait until the Grafana pod is up and running
 
. Obtain the URL to the Grafana UI:
+
----
$ GRAFANA_URL=https://$(oc get route grafana-route -n user1-metrics --template='{{ .spec.host }}')
$ echo ${GRAFANA_URL}
----

. In a browser window, navigate to the Grafana application. Log in with your OpenShift admin credentials. Expect to see the Grafana home page:
+
image::images/grafana-home-page.png[]

. In order to create a new Dashboard in Grafana, you need to log in. The Grafana admin username and password are stored in the `grafana-admin-credentials` secret.
+
----
$ echo $(oc get secret grafana-admin-credentials -n user1-metrics --template='{{ .data.GF_SECURITY_ADMIN_USER }}' | base64 -d)
$ echo $(oc get secret grafana-admin-credentials -n user1-metrics --template='{{ .data.GF_SECURITY_ADMIN_PASSWORD }}' | base64 -d)
----

. Click on the image:images/grafana-login.png[] button on the bottom of the left menu to log in into Grafana. Enter the admin user name and password. 

. Click on the image:images/grafana-configuration.png[] button on the left to view the datasources defined in the Grafana instance. Expect to see two datasources:
+
image::images/grafana-datasources.png[]

. Click on the image:images/grafana-dashboards.png[] button on the left, and select `Manage`. On the _Dashboards_ page, select _New Dashboard_ to create a new dashboard.
+
image::images/grafana-new-dashboard.png[]

. Click on the _Add new panel_ button in the empty panel to start designing the first panel of the dashboard.
+
image::images/grafana-new-dashboard-add-query.png[]

. The panel will display the memory usage as reported by the application JVM.
* Make sure the default datasource is selected.
* In the query text box, enter the Prometheus query to obtain the data. The memory usage metrics from the incident service application are exposed under the name `base_memory_` and `vendor_memory_`. As many applications could be exposing metrics under that name, you need to filter on the incident service. Enter the following query in the text box to get the data for the committed heap memory:
+
----
base_memory_committedHeap_bytes{job="incident-service"}
----
* By default the legend of the data will be the full name of the metric, including all the tags. To make it more readable, enter the following in the legend text box:
+
----
Committed Heap {{pod}}
----
+
Notice that `{{pod}}` resolves to the `pod` tag of the metric.
* Click the image:images/grafana-add-query.png[] button to add another query. Enter `base_memory_usedHeap_bytes{job="incident-service"}` in the query box, and `Used Heap {{pod}}` in the legend.
* Add more queries to capture all the memory metrics exposed by the application.
+
image::images/grafana-memory-queries.png[]

. In the right side panel, select the `Panel` tab ans scroll down to the `Axes` section. Set the unit of the left Y-axis to `bytes(IEC)` (in the `data (IEC)` category).
+
image:images/grafana-axis-bytes.png[]
. In the `Panel Title` tab at the top of the `Panel`, enter `Memory Usage JVM`.
. Click on the image:images/grafana-back.png[] icon to go back to the main dashboard page.
. Click on the timeline button on the top left and select image:images/grafana-last-30-minutes.png[] to display the data from the last 30 minutes.
+
image::images/grafana-dashboard-first-panel.png[]
. At this point is a good idea to save your dashboard. Name the dashboard `Inventory Service`.

. The data shown in the panel is the memory consumption for the application as reported by the JVM. Another interesting metric is the pod memory consumption from the OpenShift cluster point of view.
* Click on the image:images/grafana-new-panel.png[] to create a new panel.
* Select the `OpenShift Monitoring` datasource.
* In the _Query_ text box, enter the following query:
+
----
sum(container_memory_working_set_bytes{pod=~'incident-service-.*',namespace='user1-metrics',container='',}) BY (pod, namespace)
----
+
This returns the memory consumption for the incident service pods.
* Enter `{{pod}}` as the legend.
* Use bytes as data unit for the left Y-Axis, and name the panel `Memory Usage OpenShift`.
+
image::images/grafana-dashboard-memory-usage.png[]

. Continue to build the dashboard. Here are some suggestions for more panels:
* CPU usage as reported by the JVM
** Queries: `base_cpu_processCpuLoad_percent{job="incident-service"}`, `vendor_cpu_systemCpuLoad_percent{job="incident-service"}`
* CPU usage as reported by OpenShift:
** Query: `(pod:container_cpu_usage:sum{namespace="user1-metrics", pod=~"incident-service-.*"})*1000`
* Garbage Collection rate per minute:
** Query: `rate(base_gc_total{job="incident-service"}[1m])*60`
* Garbage Control Duration in milliseconds
** Query: `(rate(base_gc_time_total_seconds{job="incident-service"}[1m])/rate(base_gc_total{job="incident-service"}[1m]))*1000`
* Application REST Requests throughput:
** Query: `incident_service:rest:total:rate1m`
** Legend: ``{{pod}} {{method}}``
+
This query leverages the calculated time series you defined in the previous section.
* Application REST Requests latency in milliseconds:
** Query: `(incident_service:rest:elapsed_time:rate1m/incident_service:rest:total:rate1m)*1000`
** Legend: `{{pod}} {{method}}`
* Persistence operation throughput:
** Query: `incident_service:repository:total:rate1m`
** Label: `{{pod}} {{operation}}`
* Persistence request latency in milliseconds:
** Query: `(incident_service:repository:elapsed_time:rate1m/incident_service:repository:total:rate1m)*1000`
** Label: `{{pod}} {{operation}}`

* Datasource pool usage:
** Queries: `vendor_agroal_max_used_count{job="incident-service"}`, `vendor_agroal_active_count{job="incident-service"}`, `vendor_agroal_available_count{job="incident-service"}`

. The incident service is consuming messages from the Kafka broker. A good metric to measure whether the application can actually cope with the message is the lag on the topics. The lag is an indication of how far the application is behind on consuming messages from the topics it is consuming from. A consistently growing value for the lag is a sign that the application needs to be scaled up as it cannot handle the flow of messages. +
Message lag metrics per topic and consumer group can be obtained from the Kafka broker, more specifically from the _Kafka Exporter_ service deployed as part of the AMQ Streams cluster. +
* Add the Kafka pods as discovery target for Prometheus by deploying a _PodMonitor_ custom resource:
+
----
$ echo '
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: kafka-resources-metrics
  labels:
    app: strimzi
spec:
  selector:
    matchExpressions:
      - key: "strimzi.io/kind"
        operator: In
        values: ["Kafka", "KafkaConnect", "KafkaConnectS2I", "KafkaMirrorMaker", "KafkaMirrorMaker2"]
  namespaceSelector:
    matchNames:
      - user1-metrics
  podMetricsEndpoints:
  - path: /metrics
    port: tcp-prometheus
    relabelings:
    - separator: ;
      regex: __meta_kubernetes_pod_label_(strimzi_io_.+)
      replacement: $1
      action: labelmap
    - sourceLabels: [__meta_kubernetes_namespace]
      separator: ;
      regex: (.*)
      targetLabel: namespace
      replacement: $1
      action: replace
    - sourceLabels: [__meta_kubernetes_pod_name]
      separator: ;
      regex: (.*)
      targetLabel: kubernetes_pod_name
      replacement: $1
      action: replace
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      separator: ;
      regex: (.*)
      targetLabel: node_name
      replacement: $1
      action: replace
    - sourceLabels: [__meta_kubernetes_pod_host_ip]
      separator: ;
      regex: (.*)
      targetLabel: node_ip
      replacement: $1
      action: replace
' | oc create -f - -n user1-metrics
----
* Verify that Prometheus discovered the Kafka cluster pods. In the Prometheus UI, navigate to _Status -> Target_:
+
image::images/prometheus-kafka-target.png[]
* In the Grafana dashboard, create a panel to visualize the message lag of the `topic-incident-command` topic for the `incident service` consumer group.
** Query: `sum(kafka_consumergroup_lag{consumergroup="incident-service", topic="topic-incident-command"})`
** Optionally you can also add queries for the rate of messages added to the queue and the rate of consumption by the incident service: `sum(rate(kafka_topic_partition_current_offset{topic="topic-incident-command"}[1m]))` and `sum(delta(kafka_consumergroup_current_offset{consumergroup="incident-service", topic="topic-incident-command"}[1m])/60)` respectively.

. At this point, your dashboard might look like:
+
image::images/grafana-dashboard-complete.png[]

. When you're satisfied with your dashboard you can export the dashboard.
* Click on the image:images/grafana-share-dashboard.png[] button on the top left of the dashboard page.
* On the _Share_ dialog window, select the _Export_ tab and click _Save to file_.
+
image::images/grafana-export-dashboard.png[]
* Save the dashboard to your local filesystem. 

=== Add the Dashboard to the Emergency Response Application

You are ready now to import the dashboard into the Grafana instance of the Emergency Response application.

In the first lab of this course you installed the Emergency Response application. The application contains a monitoring stack, which is deployed in the `user1-er-metrics` namespace.

In this section of the lab you will import the dashboard you created into the Grafana instance of the Emergency Response application.

. Deploy the Incident Service image to the OpenShift cluster, tagged for the incident service imagestream:
+
----
$ REGISTRY_URL=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')
$ podman tag ${REGISTRY_URL}/user1-metrics/incident-service:metrics ${REGISTRY_URL}/user1-er-demo/incident-service:metrics
$ podman login -u $(oc whoami) -p $(oc whoami -t) ${REGISTRY_URL}
$ podman push ${REGISTRY_URL}/user1-er-demo/incident-service:metrics
----

. Patch the deploymentconfig of the incident service to point to the new image. This will force a redeployment of the incident service application. 
+
----
$ oc patch dc incident-service --type='json' -p '[{"op": "replace", "path": "/spec/triggers/1/imageChangeParams/from/name", "value": "incident-service:metrics"}]' -n user1-er-demo
----

. Label the service resource of the incident service so that the service pods can be discovered by Prometheus.
+
----
$ oc label service incident-service -n user1-er-demo monitoring=prometheus
----

. Obtain the URL to the Prometheus application:
+
----
$ PROMETHEUS_URL=https://$(oc get route prometheus-route -n user1-er-monitoring --template='{{ .spec.host }}')
$ echo ${PROMETHEUS_URL}
----

. In a browser window, navigate to the Prometheus application. In the Prometheus UI, verify that the incident service is added to the discovered targets.
+
image::images/prometheus-erdemo-targets.png[]

. Deploy the Prometheus rules for the incident service:
+
----
$ echo "
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: incident-service-prometheus-rules
spec:
  groups:
    - name: incident-service.rules
      rules:
        - record: incident_service:rest:elapsed_time:rate1m
          expr: rate(base_REST_request_elapsedTime_seconds{job=\"incident-service\"}[1m])
        - record: incident_service:rest:total:rate1m
          expr: rate(base_REST_request_total{job=\"incident-service\"}[1m])
        - record: incident_service:repository:elapsed_time:rate1m
          expr: rate(application_IncidentService_repository_elapsedTime_seconds{job=\"incident-service\"}[1m])
        - record: incident_service:repository:total:rate1m
          expr: rate(application_IncidentService_repository_total{job=\"incident-service\"}[1m])
" | oc create -f - -n user1-er-monitoring
----

. Verify in the Prometheus UI that the rules file is added to the Prometheus rules:
+
image::images/prometheus-erdemo-incident-service-rules.png[]

. A dashboard can be added to a Grafana instance deployed with the application monitoring operator with a _GrafanaDashboard_ custom resource.
* Create a skeleton GrafanaDashboard custom resource file:
+
----
$ echo "
apiVersion: integreatly.org/v1alpha1
kind: GrafanaDashboard
metadata:
  name: incident-service-dashboard
  labels:
    monitoring-key: user1-application-monitoring
spec:
  name: incident-service-dashboard.json
  json: |
    {}
" | tee /tmp/incident-service-grafana-dashboard.yml
----
* Open the file in a text editor. +
Copy the contents of the incident service dashboard JSON file you exported from Grafana earlier and paste the contents in the GrafanaResource file. +
Adjust the indentation in order to make the file a valid YAML file:
+
----
apiVersion: integreatly.org/v1alpha1
kind: GrafanaDashboard
metadata:
  name: incident-service-dashboard
  labels:
    monitoring-key: user1-application-monitoring
spec:
  name: incident-service-dashboard.json
  json: |
    {
      "annotations": {
        "list": [
          {
            "builtIn": 1,
            "datasource": "-- Grafana --",
            "enable": true,
            "hide": true,
            "iconColor": "rgba(0, 211, 255, 1)",
            "name": "Annotations & Alerts",
            "type": "dashboard"
          }
        ]
      },
      [...]
    }  
----
* The dashboard metrics gathered from OpenShift are filtered on the namespace. In the custom resource file, search for occurrences of `user1-metrics` and replace with `user1-er-demo`:
+
----
[...]
          "targets": [
            {
              "expr": "sum(container_memory_working_set_bytes{pod=~'incident-service-.*',namespace='user1-er-demo',container='',}) BY (pod, namespace)",
              "format": "time_series",
              "interval": "",
              "intervalFactor": 1,
              "legendFormat": "{{pod}}",
              "refId": "A"
            }
          ],
[...]
          "targets": [
            {
              "expr": "(pod:container_cpu_usage:sum{namespace='user1-er-demo', pod=~'incident-service-.*''})*1000",
              "format": "time_series",
              "intervalFactor": 1,
              "legendFormat": "{{pod}}",
              "refId": "A"
            }
          ],
----
* Save the file.
* Deploy the GrafanaDashboard custom resource:
+
----
$ oc create -f /tmp/incident-service-grafana-dashboard.yml -n user1-er-monitoring
----
+
.Output
----
grafanadashboard.integreatly.org/incident-service-dashboard created
----

. Obtain the URL to the Grafana UI:
+
----
$ GRAFANA_URL=https://$(oc get route grafana-route -n user1-er-metrics --template='{{ .spec.host }}')
$ ECHO ${GRAFANA_URL}
----

. In a browser window, navigate to the Grafana application. Log in with your OpenShift admin credentials.

. In the Grafana home page, click on the image:images/grafana-dashboards.png[] button on the left and select _Manage_. Expect to see the Incident Service dashboard in the list of available dashboards.
+
image::images/grafana-erdemo-dashboards.png[]

. Click on the _Incident Service_ link to open the dashboard.
+
image::images/grafana-erdemo-incident-service-dashboard.png[]

. Run a simulation on the Emergency Response application. Refer to the first lab of this course for instructions to do so.

. Check the incident service Grafana dashboard. Notice the metrics produced by the application. Also notice that the application is very much at ease with the load produced by the demo.
+
image::images/grafana-dashboard-running.png[]

== Optional Lab: Service Level Indicators and Service Level Objectives

In the previous sections of the lab you have learned that it is pretty straightforward to obtain useful metrics from a Quarkus application running on OpenShift, be it by leveraging base metrics produced by Quarkus itself and its extensions, or by explicitly instrumenting the application code with MicroProfile Metrics annotations.

However, in some cases the kind of metrics produced are not sufficient to be able for instance to check that service level agreements are being met. So for example rather than collecting metrics on the average latency for HTTP requests, the teams responsible for the application in production might be more interested in the distribution of the latency.

In this context the terms SLI (Service Level Indicator) and SLO (Service Level Objective) are often used. Refer to https://www.openshift.com/blog/monitoring-services-like-an-sre-in-openshift-servicemesh for an excellent discussion on the subject. +
An example of an SLI could be the latency of a HTTP request. The SLO for this SLI could be something like: 99.99% of the HTTP must have a latency lower than 150ms. 

The kind of metrics we collected from the incident service do not allow to deduce the information necessary to verify that the SLO is met or not, as it does not produce any information on the distribution of the data. However, MicroProfile Metrics supports the metric type `Timer`, which calculates quantiles over a sliding time window.

So how can you instrument the incident service application in order to produce histogram metrics on HTTP REST requests? You could of course instrument the application code and decorate the REST endpoints with a MicroProfile Metrics annotation, but that is not very precise, as you would only measure a fraction of the total request.

The extension mechanism of Quarkus comes to the rescue here. The role of Quarkus extensions is to leverage Quarkus paradigms to integrate seamlessly a library or additional functionality into the Quarkus architecture, making use of the build-time optimization and augmentation that Quarkus offers. Refer to https://quarkus.io/guides/building-my-first-extension and https://quarkus.io/guides/writing-extensions#extension-metrics for more details about Quarkus extensions and how to write them.

In this section of the lab you build a custom extension which replaces the HTTP metrics which are produced by the RestEasy extension and which use a MicroProfile `SimpleTimer` by an alternative implementation which uses a MicroProfile Metrics `Timer` instead. Then you will leverage this data to build a dashboard that visualizes the SLO for the REST calls to the incident service.

=== Build the `rest-metrics` Quarkus Custom Extension.

. Clone the source code of the Quarkus extension:
+
----
$ git clone https://github.com/gpte-cloud-native-advanced/quarkus-rest-metrics.git
----

. Import the source code into your IDE of choice.

. Review the code of the extension. A Quarkus extension project typically is a multi-module Maven project, with a module for the deployment phase and one for the runtime phase.
* The `RestMetricsConfig` class in the deployment module defines a configuration property named `rest-metrics.metrics.enabled` with default value equal to false.
+
----
@ConfigRoot(name = "rest-metrics", phase = BUILD_TIME)
public class RestMetricsConfig {

    /**
     * Whether or not JAX-RS metrics should be enabled if the Metrics capability is present and Vert.x is being used.
     */
    @ConfigItem(name = "metrics.enabled", defaultValue = "false")
    public boolean metricsEnabled;

}
----
* The `RestMetricsProcessor` class contains the `BuildSteps` that will be executed during the deployment phase of the application. The implementation is fairly simple: if the metrics configuration property is set to true, and the metrics capability is present - meaning the application contains the `quarkus-smallrye-metrics` extension, then a `ResteasyJaxrsProviderBuildItem` build item will be produced. A build item is a value that will be used in a later phase. In this case, an instance of `QuarkusJaxRsMetricsFilter` is added to the RestEasy Jax-RS stack.
+
----
public class RestMetricsProcessor {

    private static final Logger log = Logger.getLogger("io.quarkus.rest-metrics");

    private static final String FEATURE = "rest-metrics";

    @BuildStep
    void enableMetrics(RestMetricsConfig buildConfig, BuildProducer<ResteasyJaxrsProviderBuildItem> jaxRsProviders, Capabilities capabilities) {
        if (buildConfig.metricsEnabled && capabilities.isCapabilityPresent(Capability.METRICS.getName())) {
            if (capabilities.isCapabilityPresent(Capability.SERVLET.getName())) {
                // if running with servlet, do nothing
                log.warn("Running with servlet, rest metrics are not enabled");
            } else {
                jaxRsProviders.produce(
                        new ResteasyJaxrsProviderBuildItem("com.redhat.emergency.response.quarkus.rest.metrics.QuarkusJaxRsMetricsFilter"));
            }
        }
    }

    @BuildStep
    FeatureBuildItem feature() {
        return new FeatureBuildItem(FEATURE);
    }

}
----
* The `QuarkusJaxRsMetricsFilter` contains the implementation of the filter. The filter registers the metric if it does not yet exists, and updates the timer instance for every invocation. Notice the type of the metric, which is a MicroProfile Metrics `Timer`. Also, the metric is registered with the `APPLICATION` metric registry, rather than the `BASE` registry.
+
----
    private void finishRequest(Long start, Class<?> resourceClass, Method resourceMethod) {
        long value = System.nanoTime() - start;
        MetricID metricID = getMetricID(resourceClass, resourceMethod);

        MetricRegistry registry = MetricRegistries.get(MetricRegistry.Type.APPLICATION);
        if (!registry.getMetadata().containsKey(metricID.getName())) {
            // if no metric with this name exists yet, register it
            Metadata metadata = Metadata.builder()
                    .withName(metricID.getName())
                    .withDescription(
                            "The distribution of the invocation time of this RESTful resource method since the start of the server.")
                    .withUnit(MetricUnits.NANOSECONDS)
                    .build();
            registry.timer(metadata, metricID.getTagsAsArray());
        }
        registry.timer(metricID.getName(), metricID.getTagsAsArray())
                .update(value, TimeUnit.NANOSECONDS);
    }
----
. Build the extension, and publish to your local Maven repository:
+
----
$ mvn clean install
----

. Add a dependency to the runtime module of the extension to the `pom.xml` file of the incident service.
+
----
    <dependency>
      <groupId>com.redhat.emergency.response</groupId>
      <artifactId>quarkus-rest-metrics</artifactId>
      <version>1.0-SNAPSHOT</version>
    </dependency>
----

. Enable the metrics for the extension in the `application.properties` file of the incident service. Disable the RestEasy extension metrics.
+
----
quarkus.resteasy.metrics.enabled=false
quarkus.rest-metrics.metrics.enabled=true
----

. Build the application, create an image and push the image to the OpenShift registry.
+
----
$ mvn clean package
$ REGISTRY_URL=$(oc get route default-route -n openshift-image-registry --template='{{ .spec.host }}')
$ podman build -f docker/Dockerfile -t ${REGISTRY_URL}/user1-metrics/incident-service:metrics .
$ podman login -u $(oc whoami) -p $(oc whoami -t) ${REGISTRY_URL}
$ podman push ${REGISTRY_URL}/user1-metrics/incident-service:metrics
----

. This causes a redeployment of the incident service pod in the `user1-metrics` namespace. Check the logs of the new pod to verify that the extension has been loaded by the Quarkus runtime. Look for the name `rest-metrics` in the installed features:
+
----
2020-07-11 08:24:54,101 INFO  [io.quarkus] (main) incident-service-quarkus 1.0.0-SNAPSHOT on JVM (powered by Quarkus 1.5.2.Final) started in 15.282s. Listening on: http://0.0.0.0:8080
2020-07-11 08:24:54,102 INFO  [io.quarkus] (main) Profile prod activated. 
2020-07-11 08:24:54,102 INFO  [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, jdbc-postgresql, mutiny, narayana-jta, rest-metrics, resteasy, resteasy-mutiny, smallrye-health, smallrye-metrics, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]
----

=== Service Level Objective Dashboard

Now you can build a dashboard panel using the metrics exposed by the extension.

. The REST metrics are created lazily, so you need to invoke the REST API to activate them. +
Create an incident by calling the REST API of the incident service.
+
----
$ echo '
{
  "lat": 34.98125,
  "lon": -77.84121,
  "numberOfPeople": 5,
  "medicalNeeded": true,
  "victimName": "Jane Foo",
  "victimPhoneNumber": "(458) 741-45823)"
}
' | tee /tmp/incident.json
$ INCIDENT_SERVICE_URL=$(oc get route incident-service -n user1-metrics --template='{{ .spec.host }}')
$ curl -v -X POST -H "Content-type: application/json" -d @/tmp/incident.json http://${INCIDENT_SERVICE_URL}/incidents
----
+
Repeat the `curl` command a couple of times.

. In the Prometheus UI in the `user1-metrics` namespace, verify that the new metrics are available:
+
image::images/prometheus-extension-metrics.png[]

. In the Grafana UI, notice that the dashboard panels for the REST metrics are not longer functional. This is expected, as the original metrics are no longer produced.

. Edit the panel for the REST calls throughput. Replace the query with:
+
----
sum(application_REST_request_one_min_rate_per_second{job='incident-service', method=~'createIncident.*'})
----
+
Notice that the query filters on the method, to only take into account the REST calls to create an incident. Also, the query takes the sum of the time series, so that if you scale out the incident service pods, the total throughput over all the pods will be returned.
+
image::images/grafana-incident-service-throughput.png[]

. Create a new panel for the REST calls latency or edit the existing one. The panel should show the 50th, 90th and 99th percentiles of REST calls to create an incident.
* The percentiles (or quantiles) are produced by the MicroProfile Metrics `Timer` type. To plot the 50th percentile, use the following query: 
+
----
(application_REST_request_seconds{job='incident-service', quantile='0.5', method=~'createIncident.*'})*1000
----
+
The query filters on the REST calls to create an incident. Notice that there is no aggregation, so if you scale out the pods, you will see one graph per pod. This makes sense, as statistically speaking aggregating (e.g. taking an average) quantiles would produce non-sensical data.
* Repeat for the `0.95` and `0.99` quantiles.
* You can add a graph with a fixed value that represents the SLO. Let's say that the SLO defines that 99% of all requests need to be served within 30 milliseconds. In that case you can add a query with value `30` to the panel.
+
image:images/grafana-panel-quantiles.png[]

. To import your modified dashboard into the Grafana instance of the Emergency Response application: export the dashboard, edit the GrafanaDashboard custom resource file you created earlier with the new dashboard JSON, redeploy the GrafanaDashboard datasource in the `user1-er-metrics` namespace.

== Tear Down the Development Environment

In order to free up resources on the OpenShift cluster, you can tear down the environment you deployed in the first step of the lab. +
To do so, execute the following Ansible commands:

----
$ ansible-playbook -i inventories/inventory playbooks/incident_service.yml -e project_admin=user1 -e project_name=metrics -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/postgresql.yml -e project_admin=user1 -e project_name=metrics -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/kafka_topics.yml -e project_admin=user1 -e project_name=metrics -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/kafka_cluster.yml -e project_admin=user1 -e project_name=metrics -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/amq_streams_operator.yml -e project_admin=user1 -e project_name-metrics -e ACTION=uninstall
$ ansible-playbook -i inventories/inventory playbooks/monitoring.yml -e project_admin=user1 -e project_name=metrics -e namespace_monitoring=user1-metrics -e ACTION=uninstall
----